{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "links_dir = BASE_DIR+\"\\\\data\\\\game_links.txt\"\n",
    "csv_dir = BASE_DIR+\"\\\\data\\\\bgg_csv100.csv\"\n",
    "\n",
    "###UNCOMMENT IF USING PREVIOUSLY EXTRACTED LINKS\n",
    "#with open(links_dir) as f:\n",
    "#    lines = f.read()\n",
    "#    game_links=lines.split(',')\n",
    "\n",
    "url_page=\"https://boardgamegeek.com/browse/boardgame/page/\"\n",
    "url_root=\"https://boardgamegeek.com\"\n",
    "\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially thought that we're supposed to collect information about 50k games, so the solution here doesn't involve Selenium due to how long it would take.\n",
    "\n",
    "The process:\n",
    "First we extract the links for each game via \"extract_links\" function (with the ability to decide the start and finish page).\n",
    "\n",
    "Then we \"extract soups\" via \"extract_soups\" function by going to every single link we've collected and getting a reponse through the request.get() function. We take the response and convert it into a readable string via Beautiful Soup. In BGG's case what we receive is a javascript code. \n",
    "\n",
    "Then through \"extract_data_into_dataframe\" we first disassemble the javascript block in order to eliminate the unneccessary parts (parts that could still contain potential keywords but without any useful info). We then go through the chunks of code and keep looking for our very specific keywords that exactly the place where our information resides. We then clean that string and the result is the information we were looking for. \n",
    "This function, after collecting each bit of information into a separate list combines them all into a single dataframe. Then we save the dataframe into a csv file, completeting the process.\n",
    "\n",
    "The \"main\" function is simply an automation of the whole process, but each step can be called upon individually depending on the need. For example if we've already collected the links then we can skip that part altogether by supplying the links manually (requires uncommenting in the 2nd block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GATHERS LINKS OF EACH GAME\n",
    "\n",
    "def extract_links(page_start,page_finish):\n",
    "    page_num = page_start\n",
    "    max_page = page_finish\n",
    "    num_of_links = ((max_page-page_num)+1)*100 #there are 100 game links per BGG page\n",
    "    game_links = []\n",
    "\n",
    "    while(page_num<=max_page):\n",
    "        next_page=url_page+str(page_num)\n",
    "        response = requests.get(next_page,headers=user_agent)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        main_table = soup.find(\"table\", attrs={\"class\":\"collection_table\"})\n",
    "        game_rows = main_table.find_all(\"tr\", attrs={\"id\":\"row_\"})\n",
    "\n",
    "        #finding and collecting the actual links\n",
    "        for game in game_rows:\n",
    "            game_links.append(game.find(\"a\", attrs={\"class\":\"primary\"})['href'])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{len(game_links)}/{num_of_links} links collected\")\n",
    "\n",
    "        page_num+=1\n",
    "        \n",
    "    return game_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SAVES LINKS IN links_save_dir \n",
    "\n",
    "def save_links(game_links):\n",
    "    with open(links_dir, \"w\") as f:\n",
    "        for game in game_links:\n",
    "            if(game == game_links[0]):\n",
    "                f.write(game)\n",
    "            else:\n",
    "                f.write(','+game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PRINTS CURRENT PROGRESS OF SOUP EXTRACTION\n",
    "\n",
    "#since extraction can take many hours for over 50k pages, for sake of convinience we've added a timer and updates for the progress\n",
    "\n",
    "def soup_progress(total, len_soups,len_links):\n",
    "    minutes = int((total/60)%60)\n",
    "    hours = int((total/60)/60)\n",
    "    seconds = int(total % 60)\n",
    "\n",
    "    #if the number is a double digit one such as 12, leave it as is, if it's single digit like 7 then we turn it into 07\n",
    "    if(seconds>9):\n",
    "        seconds_modifier=\"\"\n",
    "    else:\n",
    "        seconds_modifier=\"0\"\n",
    "\n",
    "    if(minutes>9):\n",
    "        minutes_modifier=\"\"\n",
    "    else:\n",
    "        minutes_modifier=\"0\"\n",
    "\n",
    "    if(hours>9):\n",
    "        hours_modifier=\"\"\n",
    "    else:\n",
    "        hours_modifier=\"0\"\n",
    "\n",
    "    percent = (len_soups/len_links)*100\n",
    "    percent = float(\"{:.2f}\".format(percent))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(f\"{len_soups}/{len_links} soups, {hours_modifier}{hours}:{minutes_modifier}{minutes}:{seconds_modifier}{seconds} time passed, {(percent)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GOES TO LINKS AND CREATES SOUPS TO EXTRACT DATA FROM \n",
    "\n",
    "def extract_soups(game_links):\n",
    "    soups=[]\n",
    "    total=0\n",
    "\n",
    "    for link in game_links:\n",
    "        start=time.time() #Starting the timer for soup_progress\n",
    "        \n",
    "        next_page=url_root+link\n",
    "        response = requests.get(next_page)\n",
    "        soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "        soups.append(soup)\n",
    "        \n",
    "        end = time.time()#Ending the timer for soup_progress\n",
    "        total+=(end-start)#Total time elapsed so far\n",
    "        soup_progress(total, len(soups), len(game_links))\n",
    "\n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETS MAX SCRIPT WHERE WE CAN FIND THE USEFUL INFO + CREATES A DATAFRAME FROM THE DATA\n",
    "\n",
    "def extract_data_into_dataframe(soups): \n",
    "    years=[]\n",
    "    min_players=[]\n",
    "    max_players=[]\n",
    "    min_times=[]\n",
    "    max_times=[]\n",
    "    min_ages=[]\n",
    "    weights=[]\n",
    "    ranks=[]\n",
    "    designers=[]\n",
    "    artists=[]\n",
    "    publishers=[]\n",
    "    owned=[]\n",
    "    best_num_players=[]\n",
    "    num_of_ratings=[]\n",
    "    num_of_comments=[]\n",
    "    names=[]\n",
    "    ratings=[]\n",
    "    types=[]\n",
    "    error_counter = 0\n",
    "    error_indices = []\n",
    "\n",
    "    for i in range(len(soups)):\n",
    "        corr_scripts = soups[i](\"script\")\n",
    "        max_script_index = 0 \n",
    "        max_curr = 0\n",
    "\n",
    "        if(len(corr_scripts)==0): ###if we got a broken/unexpected soup with an error\n",
    "            error_counter+=1\n",
    "            error_indices.append(i)\n",
    "            continue\n",
    "                    \n",
    "        ###EXTRACT THE BOARDGAME NAME\n",
    "        name=soups[i].find(\"title\").string.replace(\" | Board Game | BoardGameGeek\",\"\").strip()\n",
    "        names.append(name)    \n",
    "        \n",
    "        clear_output(wait=True)        \n",
    "\n",
    "        \n",
    "        for script in corr_scripts:\n",
    "            if(len(script.get_text())>max_curr):\n",
    "                max_curr = len(script.get_text())\n",
    "                max_script_index = corr_scripts.index(script)\n",
    "\n",
    "        script_for_data = corr_scripts[max_script_index]\n",
    "\n",
    "        data_string = script_for_data.string\n",
    "        data_string=data_string.rsplit(',')\n",
    "        isRanked=False\n",
    "        isDesignerFound=False\n",
    "\n",
    "        for data in data_string:\n",
    "            \n",
    "            ###EXTRACT THE RELEASE YEAR\n",
    "            if(f'yearpublished\":' in data):\n",
    "                year=data.replace(f'\"', \"\")\n",
    "                year=year.replace(f'yearpublished:',\"\").strip()\n",
    "                \n",
    "                if(year == '0'): \n",
    "                    year='NULL'\n",
    "                years.append(year)                                \n",
    "\n",
    "            ###EXTRACT THE MINIMUM AMOUNT OF PLAYERS\n",
    "            if(f'minplayers\":' in data):\n",
    "                min_player=data.replace(f'\"', \"\")\n",
    "                min_player=min_player.replace(f'minplayers:',\"\").strip()\n",
    "\n",
    "                if(min_player == '0'):\n",
    "                    min_player='NULL'\n",
    "                min_players.append(min_player)\n",
    "\n",
    "            ###EXTRACT THE MAXIMUM AMOUNT OF PLAYERS\n",
    "            if(f'maxplayers\":' in data):\n",
    "                max_player=data.replace(f'\"', \"\")\n",
    "                max_player=max_player.replace(f'maxplayers:',\"\").strip()\n",
    "\n",
    "                if(max_player == '0'):\n",
    "                    if((min_players[-1])=='NULL'):\n",
    "                        max_player='NULL'\n",
    "                    else:\n",
    "                        max_player=min_players[-1]\n",
    "                max_players.append(max_player) \n",
    "\n",
    "             ###EXTRACT THE MINIMUM PLAY TIME\n",
    "            if(f'minplaytime\":' in data):\n",
    "                min_time=data.replace(f'\"', \"\")\n",
    "                min_time=min_time.replace(f'minplaytime:',\"\").strip()\n",
    "\n",
    "                if(min_time == '0'):\n",
    "                    min_time='NULL'\n",
    "                min_times.append(min_time)\n",
    "\n",
    "             ###EXTRACT THE MAXIMUM PLAY TIME\n",
    "            if(f'maxplaytime\":' in data):\n",
    "                max_time=data.replace(f'\"', \"\")\n",
    "                max_time=max_time.replace(f'maxplaytime:',\"\").strip()\n",
    "\n",
    "                if(max_time == '0'):\n",
    "                    max_time='NULL'\n",
    "                max_times.append(max_time)                     \n",
    "\n",
    "                \n",
    "             ###EXTRACT THE RECOMMENDED MINIMUM AGE\n",
    "            if(f'minage\":' in data):\n",
    "                min_age=data.replace(f'\"', \"\")\n",
    "                min_age=min_age.replace(f'minage:',\"\").strip()\n",
    "\n",
    "                if(min_age == '0'):\n",
    "                    min_age='NULL'\n",
    "                min_ages.append(min_age) \n",
    "\n",
    "             ###EXTRACT THE AVERAGE GAME COMPLEXITY LEVEL/'WEIGHT'\n",
    "            if(f'avgweight\":' in data):\n",
    "                weight=data.replace(f'\"', \"\")\n",
    "                weight=weight.replace(f'avgweight:',\"\").strip()\n",
    "\n",
    "                if(weight == '0'):\n",
    "                    weight='NULL'\n",
    "                weights.append(weight)\n",
    "\n",
    "             ###EXTRACT THE RANK\n",
    "            if(f'rank\":' in data and isRanked==False):\n",
    "                rank=data.replace(f'\"', \"\")\n",
    "                rank=rank.replace(f'rank:',\"\").strip()\n",
    "                isRanked = True\n",
    "\n",
    "                if(rank == '0'):\n",
    "                    rank='NULL'\n",
    "                ranks.append(rank)\n",
    "                \n",
    "            \n",
    "             ###EXTRACT THE DESIGNER\n",
    "            if('links\":{' in data and isDesignerFound==False):\n",
    "                isDesignerFound=True\n",
    "                designer=data.replace(f'\"', \"\")\n",
    "\n",
    "                if('\"links\":{\"boardgamedesigner\":[{' in data):\n",
    "                    designer=designer.replace('links:{boardgamedesigner:[{name:',\"\").strip()\n",
    "                else:\n",
    "                    designer=designer.replace('links:{boardgamedesigner:[]',\"NULL\").strip()\n",
    "\n",
    "                if(designer=='(Uncredited)'):\n",
    "                    designer='NULL'\n",
    "                designers.append(designer)            \n",
    "\n",
    "             ###EXTRACT THE NUMBER OF PEOPLE WHO OWN THE GAME\n",
    "            if('numowned' in data):\n",
    "                num_owned=data.replace(f'\"', \"\")\n",
    "                num_owned=num_owned.replace('numowned:',\"\").strip()\n",
    "                owned.append(num_owned)            \n",
    "            \n",
    "            ###EXTRACT THE AVERAGE RATING SCORE\n",
    "            if('\"average\":' in data):\n",
    "                avg_rating=data.replace(f'\"', \"\")\n",
    "                avg_rating=avg_rating.replace('average:',\"\").strip()\n",
    "\n",
    "                if(avg_rating=='0'):\n",
    "                    avg_rating='NULL'\n",
    "                ratings.append(avg_rating)            \n",
    "            \n",
    "            ###EXTRACT THE NUMBER OF PEOPLE WHO RATED\n",
    "            if('\"stats\":' in data):\n",
    "                num_of_rating=data.replace(f'\"', \"\")\n",
    "                num_of_rating=num_of_rating.replace('stats:{usersrated:',\"\").strip()\n",
    "                num_of_ratings.append(num_of_rating)\n",
    "                \n",
    "            ###EXTRACT THE NUMBER OF COMMENTS\n",
    "            if('\"numcomments\"' in data):\n",
    "                num_of_comment=data.replace(f'\"', \"\")\n",
    "                num_of_comment=num_of_comment.replace('numcomments:',\"\").strip()\n",
    "                num_of_comments.append(num_of_comment)\n",
    "                \n",
    "            ###EXTRACT THE BEST NUMBER OF PLAYERS\n",
    "            if('\"polls\":' in data):\n",
    "                num_players=data.replace(f'\"', \"\")\n",
    "                if('\"polls\":{\"userplayers\":{\"best\":[{' in data):\n",
    "                    num_players=num_players.replace('polls:{userplayers:{best:[{min:',\"\").strip()\n",
    "                else:\n",
    "                    num_players=num_players.replace('polls:{userplayers:{best:[]',\"NULL\").strip()\n",
    "\n",
    "                best_num_players.append(num_players)\n",
    "\n",
    "            ###EXTRACT THE CATEGORY\n",
    "            if('\"boardgamesubdomain\":[' in data):\n",
    "                game_type=data.replace(f'\"', \"\")\n",
    "                if('\"boardgamesubdomain\":[{' in data):\n",
    "                    game_type=game_type.replace('boardgamesubdomain:[{name:',\"\").replace(' Games',\"\").strip()\n",
    "                else:\n",
    "                    game_type=game_type.replace('boardgamesubdomain:[]',\"NULL\").strip()\n",
    "\n",
    "                types.append(game_type)\n",
    "\n",
    "            ###EXTRACT THE ARTIST    \n",
    "            if('boardgameartist\":[' in data):     \n",
    "                artist=data.replace(f'\"', \"\")\n",
    "                if('boardgameartist\":[{' in data):   \n",
    "                    artist=artist.replace('boardgameartist:[{name:',\"\").strip()\n",
    "                else:\n",
    "                    artist=artist.replace('boardgameartist:[]',\"NULL\").strip()\n",
    "\n",
    "                if(artist=='(Uncredited)'):\n",
    "                    artist='NULL'\n",
    "                artists.append(artist)\n",
    "                isArtistFound=True\n",
    "                \n",
    "            ###EXTRACT THE PUBLISHER\n",
    "            if('boardgamepublisher\":[' in data): \n",
    "                publisher=data.replace(f'\"', \"\")\n",
    "                publisher=publisher.replace('boardgamepublisher:[{name:',\"\").strip()\n",
    "\n",
    "                if(publisher=='(Unknown)'):\n",
    "                    publisher='NULL'\n",
    "                publishers.append(publisher)\n",
    "            \n",
    "    print(f\"errors={error_counter}\\n\\nyears={len(years)}\\nmin_players={len(min_players)}\\nmax_players={len(max_players)}\\nmin_times={len(min_times)}\\nmax_times={len(max_times)}\\nmin_ages={len(min_ages)}\\nweights={len(weights)}\\nranks={len(ranks)}\\ndesigners={len(designers)}\\nartists={len(artists)}\\npublishers={len(publishers)}\\nowned={len(owned)}\\nbest_num_players={len(best_num_players)}\\nnames={len(names)}\\nratings={len(ratings)}\\ntypes={len(types)}\\nnum_of_ratings={len(num_of_ratings)}\\nnum_of_comments={len(num_of_comments)}\")\n",
    "    df = pd.DataFrame({\"Name\":names,\"Rank\":ranks,\"Rating\":ratings,\"#_of_Ratings\":num_of_ratings,\"#_of_Comments\":num_of_comments,\"Year\":years,\"Min_Players\":min_players,\"Max_Players\":max_players,\"Best_Players\":best_num_players,\"Min_Time\":min_times,\"Max_Time\":max_times,\"Min_Age\":min_ages, \"Complexity\":weights,\"Copies Owned\":owned,\"Type\":types ,\"Designer\":designers,\"Artist\":artists, \"Publisher\":publishers})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df, csv_dir):   \n",
    "    df.to_csv(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############--MAIN--##############\n",
    "\n",
    "###STEP ONE, LINK GATHERING\n",
    "game_links = extract_links(page_start=1,page_finish=1) ##comment to extract links from links_load_dir\n",
    "\n",
    "###STEP TWO, SAVING THE LINKS\n",
    "save_links(game_links)\n",
    "     \n",
    "###STEP THREE, SOUP GATHERING\n",
    "soups = extract_soups(game_links)\n",
    "\n",
    "###STEP FOUR, DATA GATHERING+DATAFRAME CREATION\n",
    "df = extract_data_into_dataframe(soups)\n",
    "\n",
    "###STEP FIVE, SAVING DATAFRAME INTO CSV\n",
    "save_csv(df, csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
